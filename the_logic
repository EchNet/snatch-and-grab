Job Queue:
  Jobs are just URLs.  The Job Queue rejects duplicate URLs.

Worker:
  [startup]
  Send cold startup message.
  Read the control block
  If failed, Send startup error message and leave.
  Send warm startup message.
  Set the URL either from the command line, the queue, or the control block.
  If I have no URL, exit.
  Canonicalize the URL(?)

  [fetch]
  Look up the document record for the URL.
  If there is a document record:
    If document record was written by a different version of worker, remove it.
	Else If document has been checked within the freshness period, leave.
	Else:
	  Fetch the headers from the URL.
	  If server is unreachable:
		requeue the URL
		send a message (can cause the control
		exit
	  Update last accessed time on document record.
	  If headers match document record, exit.
	  If header request fails, log and exit.
	end
  end
  Fetch the document.
  If request failed, log and exit.

  [digest]
  Produce a digest of the document contents.
  If the digest matches that of the document record, log and exit.
  Create/update the document record.
	- URL
	- status
	- headers
	- worker version
	- digest

  [followup]
  If the digest contains information to be indexed, send messages
  If the digest contains links, append them to the queue.

Message Queue:

Logger:
  Write all messages to aggregate log.

Master Main:
  Listen to messages, maintain a model of workers.
  Schedule new workers as workers finish.
  Kill and clean up after stuck workers.
  Respond to change in error rate by adjusting sleep period and max worker count
  If a new master starts, exit.

Master Web Server:
  simple page provides:
	# of jobs in queue
	# of workers
    # of network errors in last minute, 15 seconds, 5 seconds.
	# of updates in last minute, 15 seconds, 5 seconds.
	size of index (and other search engine stats?)
  refreshes every 5 seconds

Indexer:
  Listens to index messages
  Batches them up
  Logs them, and
  Writes them into the search engine.
